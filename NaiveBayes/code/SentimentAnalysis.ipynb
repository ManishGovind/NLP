{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes for Sentiment Analysis \n",
    "\n",
    "This assignment is comprised of two parts:\n",
    "\n",
    "1. **Theory**: Solve the Naive Bayes exercises 4.1 and 4.2 from Chapter 4 in the J&M textbook. Reformulate NB to emphasize title words.\n",
    "2. **Implementation**: You will implement and experiment with various feature engineering techniques in the context of Naive Bayes models for Sentiment classification of movie reviews.\n",
    "\n",
    "We will use the NB model implemented in sklearn:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manish Kumar Govind:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Please make sure to have entered your name above.\n",
    "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of all cells). \n",
    "4. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "5. Once you've rerun everything, select File -> Download as -> PDF via LaTeX and download a PDF version *SentimentAnalysis.pdf* showing the code and the output of all cells, and save it in the same folder that contains the notebook file *SentimentAnalysis.ipynb*.\n",
    "6. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing we will see when grading!\n",
    "7. Submit **both** your PDF and notebook on Canvas.\n",
    "8. Verify your Canvas submission contains the correct files by downloading it after posting it on Canvas.\n",
    "\n",
    "*Make sure that you format your solutions to theory questions to show equations properly. We will not grade solutions that are not properly formatted. Jupyter-notebook understands Latex, alternatively you can edit in Word using its Equation editor and submit the PDF as a separate file in Canvas.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: J&M Exercise 4.1 ##\n",
    "\n",
    "\n",
    "### D =  I always like foreign films ###\n",
    "\n",
    "P(P|D) = 0.5 * 0.09 * 0.07 * 0.29 * 0.04 * 0.08 = 0.0000029232 \n",
    "\n",
    "\n",
    "P(N|D) = 0.5 * 0.16 * 0.06 * 0.06 * 0.15 * 0.11 = 0.000004752\n",
    "\n",
    "NB classifier would classify the sentence to Negative class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: J&M Exercise 4.2 ##\n",
    "Vocab  = fun , couple , love , fast , furious , shoot , fly  = {7}\n",
    "\n",
    "D = fast, couple, shoot, fly\n",
    "\n",
    "P(fast | comedy) =  (1 + 1) / (9 + 7) = 2/16\n",
    "\n",
    "P(couple | comedy) =  (2 + 1) / (9 + 7) = 3/16\n",
    "\n",
    "P(shoot | comedy) = (0 + 1) / (9 + 7) = 1/16\n",
    "\n",
    "P(fly| comedy) =  (1 + 1) / (9 + 7) = 2/16\n",
    "\n",
    "P(fast| action) =  (2 + 1) / (11 + 7) = 3/18\n",
    "\n",
    "P(couple | action) =  (0 + 1) / (11 + 7) = 1/18\n",
    "\n",
    "P(shoot | action) =  (4 + 1) / (11 + 7) = 5/18\n",
    "\n",
    "P(fly | action) = (1 + 1) / (11 + 7) = 2/18\n",
    "\n",
    "P(action|D) = 3/5 * 3/18 * 1/18 * 5/18 * 2/18 = 0.000171\n",
    "\n",
    "\n",
    "P(comedy|D) = 2/5 * 2/16 * 3/16 * 1/16 * 2/16 = 0.0000732\n",
    "\n",
    "The D belongs to action class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "\n",
    "## Theory 5111: Title is ùêæ times more important than Body \n",
    "The Naive Bayes algorithm for text categorization presented in class treats all sections of a document equally, ignoring the fact that words in the title are often more important than words in the text in determining the document category. Modify the Naive Bayes training algorithm to reflect that word occurrences in the title are $K$ times more important than word occurrences in the rest of the document for deciding the class, where $K$ is an input parameter. Describe the idea in English and include pseudocode, akin to the training pseudocode shown in class\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "### Text Classification with Na√Øve Bayes and Weighted Features\n",
    "\n",
    "When classifying a new document, we follow these steps:\n",
    "\n",
    "1. Split the document into a title and body.\n",
    "2. Calculate the likelihoods for each part separately using the p(wi | Cj) values.\n",
    "3. Combine the likelihoods with appropriate weighting based on the factor K.\n",
    "\n",
    "This process allows  to categorize new documents effectively using the Na√Øve Bayes classifier trained with weighted features.\n",
    "In text classification, we employ the Na√Øve Bayes algorithm to determine the category of a document based on its content. Here's an overview of the process and the pseudo-code for training with a weight factor K.\n",
    "\n",
    "### Training with Na√Øve Bayes\n",
    "\n",
    "**Input:**\n",
    "- Dataset of training documents D with vocabulary V.\n",
    "  \n",
    "**Output:**\n",
    "-  Parameters p(Cj) and p(wi | Cj), K - weight factor for title .\n",
    "\n",
    "1. For each category Ck:\n",
    "2. Let Dj be the subset of documents in category Cj.\n",
    "3. For each document d ‚àä Dj, remove duplicates from d.\n",
    "4. Set p(Cj) = |Dj| / |D|.\n",
    "5. Let nbj be the total number of words in the body of Document Dj.\n",
    "6. Let ntj be the total number of words in the title of Document Dj.\n",
    "7. For each word wi √é V:\n",
    "   1. Let wbji be the frequency of word wi in the body in document Dj.\n",
    "   2. Let wtji be the frequency of word wi in the title in document Dj.\n",
    "   3. Set p(wi | Cj) = (wbji + (K * wtji) + 1) / (nbj + (ntj * K) + |V|).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From documents to feature vectors\n",
    "This section illustratess the prototypical components of machine learning pipeline for an NLP task, in this case document classification:\n",
    "\n",
    "1. Read document examples (train, devel, test) from files with a predefined format:\n",
    "    - assume one document per line, usign the format \"\\<label\\> \\<text\\>\".\n",
    "\n",
    "2. Tokenize each document:\n",
    "    - using a spaCy tokenizer.\n",
    "\n",
    "3. Feature extractors:\n",
    "    - so far, just words.\n",
    "\n",
    "4. Process each document into a feature vector:\n",
    "    - map document to a dictionary of feature names.\n",
    "    - map feature names to unique feature IDs.\n",
    "    - each document is a feature vector, where each feature ID is mapped to a feature value (e.g. word occurences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from scipy import sparse\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spaCy tokenizer.\n",
    "spacy_nlp = English()\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    tokens = spacy_nlp.tokenizer(text)\n",
    "    \n",
    "    return [token.text for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples(filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename, mode = 'r', encoding = 'utf-8') as file:\n",
    "        for line in file:\n",
    "            [label, text] = line.rstrip().split(' ', maxsplit = 1)\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(tokens):\n",
    "    feats = {}\n",
    "    for word in tokens:\n",
    "        feat = 'WORD_%s' % word\n",
    "        if feat in feats:\n",
    "            feats[feat] +=1\n",
    "        else:\n",
    "            feats[feat] = 1\n",
    "    return feats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(feats, new_feats):\n",
    "    for feat in new_feats:\n",
    "        if feat in feats:\n",
    "            feats[feat] += new_feats[feat]\n",
    "        else:\n",
    "            feats[feat] = new_feats[feat]\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function tokenizes the document, runs all the feature extractors on it and assembles the extracted features into a dictionary mapping feature names to feature values. It is important that feature names do not conflict with each other, i.e. different features should have different names. Each document will have its own dictionary of features and their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2features(trainX, feature_functions, tokenizer):\n",
    "    examples = []\n",
    "    count = 0\n",
    "    for doc in trainX:\n",
    "        feats = {}\n",
    "\n",
    "        tokens = tokenizer(doc)\n",
    "        \n",
    "        for func in feature_functions:\n",
    "            add_features(feats, func(tokens))\n",
    "\n",
    "        examples.append(feats)\n",
    "        count +=1\n",
    "        \n",
    "        if count % 100 == 0:\n",
    "            print('Processed %d examples into features' % len(examples))\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts feature names to unique numerical IDs.\n",
    "\n",
    "def create_vocab(examples):\n",
    "    feature_vocab = {}\n",
    "    idx = 0\n",
    "    for example in examples:\n",
    "        for feat in example:\n",
    "            if feat not in feature_vocab:\n",
    "                feature_vocab[feat] = idx\n",
    "                idx += 1\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts a set of examples from a dictionary of feature names to values representation\n",
    "# to a sparse representation of feature ids to values. This is important because almost all feature values will\n",
    "# be 0 for most documents and it would be wasteful to save all in memory.\n",
    "\n",
    "def features_to_ids(examples, feature_vocab):\n",
    "    new_examples = sparse.lil_matrix((len(examples), len(feature_vocab)))\n",
    "    for idx, example in enumerate(examples):\n",
    "        for feat in example:\n",
    "            if feat in feature_vocab:\n",
    "                new_examples[idx, feature_vocab[feat]] = example[feat]\n",
    "                \n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation pipeline for the Naive Bayes classifier.\n",
    "\n",
    "def train_and_test(trainX, trainY, devX, devY, feature_functions, tokenizer):\n",
    "    # Pre-process training documents. \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "\n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_vocab = create_vocab(trainX_feat)\n",
    "    print('Vocabulary size: %d' % len(feature_vocab))\n",
    "\n",
    "    trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "    # Train NB model.\n",
    "    nb_model = MultinomialNB(alpha = 1.0)\n",
    "    nb_model.fit(trainX_ids, trainY)\n",
    "    \n",
    "    # Pre-process test documents. \n",
    "    devX_feat = docs2features(devX, feature_functions, tokenizer)\n",
    "    devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    # Test NB model.\n",
    "    print('Accuracy: %.3f' % nb_model.score(devX_ids, devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 28692\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.779\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "datapath = '../data'\n",
    "\n",
    "train_file = os.path.join(datapath, 'imdb_sentiment_train.txt')\n",
    "trainX, trainY = read_examples(train_file)\n",
    "dev_file = os.path.join(datapath, 'imdb_sentiment_dev.txt')\n",
    "devX, devY = read_examples(dev_file)\n",
    "\n",
    "# Specify features to use.\n",
    "features = [word_features]\n",
    "\n",
    "# Evaluate NB model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate NB model performance when using only alpha tokens. This can be done by changing the tokenizer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 25054\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.785\n"
     ]
    }
   ],
   "source": [
    "def spacy_tokenizer1(text):\n",
    "    tokens = spacy_nlp.tokenizer(text)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Keep in the tokens list only those whose text is made up solely from letters.\n",
    "    tokens = [token.text for token in tokens if token.is_alpha]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Evaluate NB model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, but lowercase all tokens before using as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 21708\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.784\n"
     ]
    }
   ],
   "source": [
    "def spacy_tokenizer2(text):\n",
    "    tokens = spacy_nlp.tokenizer(text)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Keep in the tokens list only those whose text is made up solely from letters.\n",
    "    # Return a list of lowercased token text.\n",
    "    tokens =  [token.text.lower() for token in tokens if token.is_alpha]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Evaluate NB model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, but lowercase only tokens that appear at the beginning of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 28695\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.779\n"
     ]
    }
   ],
   "source": [
    "spacy_nlp = English()\n",
    "spacy_nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "def spacy_tokenizer3(text):\n",
    "    doc = spacy_nlp(text)\n",
    "    tokens = []\n",
    "    for sent in doc.sents:\n",
    "        is_first = True\n",
    "        for token in spacy_nlp.tokenizer(sent.text):\n",
    "            if is_first :\n",
    "                tokens.append(token.text.lower())\n",
    "                is_first = False \n",
    "            else :\n",
    "                tokens.append(token.text)\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Evaluate NB model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use spacy_tokenizer2 (only alpha tokens, lowered all) and display the top 10 most frequent tokens in the vocabulary, as a list of tuples (token, frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 21708\n",
      "('WORD_the', 20105)\n",
      "('WORD_and', 9876)\n",
      "('WORD_a', 9749)\n",
      "('WORD_of', 9029)\n",
      "('WORD_to', 8182)\n",
      "('WORD_is', 6605)\n",
      "('WORD_in', 5598)\n",
      "('WORD_it', 5387)\n",
      "('WORD_i', 4773)\n",
      "('WORD_this', 4448)\n",
      "('WORD_that', 4252)\n",
      "('WORD_was', 2961)\n",
      "('WORD_with', 2693)\n",
      "('WORD_as', 2682)\n",
      "('WORD_movie', 2574)\n",
      "('WORD_for', 2556)\n",
      "('WORD_but', 2435)\n",
      "('WORD_film', 2359)\n",
      "('WORD_you', 2066)\n",
      "('WORD_on', 2061)\n"
     ]
    }
   ],
   "source": [
    "# First, count token occurrences across all examples, where features are still strings. \n",
    "def create_feature_counts(examples):\n",
    "    feature_counts = {}\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for i in examples:\n",
    "        for k,v in i.items():\n",
    "            if k in feature_counts :\n",
    "                feature_counts[k] += v\n",
    "            else :\n",
    "                feature_counts[k] = v\n",
    "                \n",
    "\n",
    "    return feature_counts\n",
    "\n",
    "# Create features for all training examples, compute feature counts \n",
    "def fcounts_from_train(trainX, feature_functions, tokenizer):\n",
    "    # Pre-process training documents. \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "\n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_counts = create_feature_counts(trainX_feat)\n",
    "    print('Vocabulary size: %d' % len(feature_counts))\n",
    "\n",
    "    return feature_counts\n",
    "\n",
    "# Return a list of the top K most frequent tokens in the vocabulary.\n",
    "def topK_tokens(vocab, k):\n",
    "    return sorted(vocab.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "\n",
    "\n",
    "vocab = fcounts_from_train(trainX, features, spacy_tokenizer2)\n",
    "stop_words = topK_tokens(vocab, 20)\n",
    "for item in stop_words:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate NB model performance when ignoring the top 20 stop words. Use spacy_tokenizer2 (only alpha tokens, lowered all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_without_stopwords(examples , stop_words):\n",
    "    feature_vocab = {}\n",
    "    idx = 0\n",
    "    for example in examples:\n",
    "        for feat in example:\n",
    "            if feat not in feature_vocab and feat not in stop_words:\n",
    "                feature_vocab[feat] = idx\n",
    "                idx += 1\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 21688\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.798\n"
     ]
    }
   ],
   "source": [
    "# Evaluation pipeline for the Naive Bayes classifier.\n",
    "\n",
    "def train_and_test(trainX, trainY, devX, devY, feature_functions, tokenizer):\n",
    "    # Pre-process training documents. \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_counts = create_feature_counts(trainX_feat)\n",
    "    stop_words = topK_tokens(vocab, 20)\n",
    "\n",
    "    # Remove from each example features that appear in the stop words list.\n",
    "    # YOUR CODE HERE.\n",
    "    ignore_words = []   # list of stop words\n",
    "    for (word ,fre) in stop_words:\n",
    "        ignore_words.append(word)\n",
    "\n",
    "    # Create vocabulary from features in training examples.\n",
    "    #feature_vocab = create_vocab(trainX_feat) \n",
    "    feature_vocab = create_vocab_without_stopwords(trainX_feat,ignore_words)                  # accuracy improved  from 78.4 to 79.8%        \n",
    "    print('Vocabulary size: %d' % len(feature_vocab))\n",
    "\n",
    "    trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "    \n",
    "    # Train NB model.\n",
    "    nb_model = MultinomialNB(alpha = 1.0)\n",
    "    nb_model.fit(trainX_ids, trainY)\n",
    "    \n",
    "    # Pre-process test documents. \n",
    "    devX_feat = docs2features(devX, feature_functions, tokenizer)\n",
    "    devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    # Test NB model.\n",
    "    print('Accuracy: %.3f' % nb_model.score(devX_ids, devY))\n",
    "    \n",
    "# Evaluate NB model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate NB model performance when ignoring words that appear less than 5 times. Use spacy_tokenizer2 (only alpha tokens, lowered all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 5553\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.811\n"
     ]
    }
   ],
   "source": [
    "def train_and_test(trainX, trainY, devX, devY, feature_functions, tokenizer):\n",
    "    # Pre-process training documents. \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_counts = create_feature_counts(trainX_feat)\n",
    "    stop_words = topK_tokens(vocab, 20)\n",
    "\n",
    "    # Remove from each example features that appear in the stop words list.\n",
    "   \n",
    "    ignore_words = []   \n",
    "    # list of  words whose frequency is less than 5\n",
    "    \n",
    "    for word , fre  in feature_counts.items():\n",
    "        if fre < 5 :\n",
    "            ignore_words.append(word)\n",
    "\n",
    "    # list of   stop words \n",
    "    for (word ,fre) in stop_words:\n",
    "        ignore_words.append(word)\n",
    "    # Create vocabulary from features in training examples.\n",
    "    #feature_vocab = create_vocab(trainX_feat) \n",
    "    feature_vocab = create_vocab_without_stopwords(trainX_feat,ignore_words)                  # accuracy improved  from 79.8 to 81.1         \n",
    "    print('Vocabulary size: %d' % len(feature_vocab))\n",
    "\n",
    "    trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "    \n",
    "    # Train NB model.\n",
    "    nb_model = MultinomialNB(alpha = 1.0)\n",
    "    nb_model.fit(trainX_ids, trainY)\n",
    "    \n",
    "    # Pre-process test documents. \n",
    "    devX_feat = docs2features(devX, feature_functions, tokenizer)\n",
    "    devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    # Test NB model.\n",
    "    print('Accuracy: %.3f' % nb_model.score(devX_ids, devY))\n",
    "    \n",
    "# Evaluate NB model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Multinomial Bayes\n",
    "*Mandatory for graduate students, optional for undergraduate students*\n",
    "\n",
    "Write code for transforming documents to features such that features are Boolean and only represent whether a word occurred in a document, as in the Binary Multinomial Naive Bayes discussed in class. Evaluate the Naive Bayes model with this feature representation, using spacy_tokenizer2 (only alpha tokens, lowered all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_features_binary(tokens):\n",
    "    feats = {}\n",
    "    for word in tokens:\n",
    "        feat = 'WORD_%s' % word\n",
    "        if feat not in feats:\n",
    "            feats[feat] = 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 4852\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.826\n"
     ]
    }
   ],
   "source": [
    "# Specify features to use.\n",
    "features = [word_features_binary]\n",
    "\n",
    "# Evaluate NB model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus points ##\n",
    "Anything extra goes here. For example, implement NB from scratch in a separate module nbayes.py and use it for the exercises above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 21708\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.8066666666666666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from  nbayes import  train_nb , test_nb\n",
    "def train_and_test_custom(trainX, trainY, devX, devY, feature_functions, tokenizer):\n",
    "    # Pre-process training documents. \n",
    " \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_vocab = create_vocab(trainX_feat)\n",
    "    print('Vocabulary size: %d' % len(feature_vocab))\n",
    "\n",
    "    trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "    # Train NB model.\n",
    "    logprior , loglikelihood = train_nb(trainX_feat,trainY,feature_vocab)\n",
    "    \n",
    "    # Pre-process test documents. \n",
    "    devX_feat = docs2features(devX, feature_functions, tokenizer)\n",
    "    devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "    acc = test_nb(devX_feat, devY , logprior, loglikelihood)\n",
    "    print(\"Accuracy:\", acc)\n",
    "train_and_test_custom(trainX, trainY, devX, devY, features, spacy_tokenizer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis ##\n",
    "Include an analysis of the results that you obtained in the experiments above. Take advantage of the Jupyter Notebook markdown language, which can also process Latex and HTML, to format your report so that it looks professional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "### Analysis of Different Experiments on Basis of Feature Engineering\n",
    "\n",
    "1. **All Tokens in Vocabulary (77.9% Accuracy):**\n",
    "   - An initial experiment considered all tokens in the vocabulary, resulting in **77.9% accuracy**.\n",
    "\n",
    "2. **Alpha Tokens Only (78.5% Accuracy):**\n",
    "   - By considering only alpha tokens (words containing alphabets), the accuracy slightly improved to **78.5%**.\n",
    "\n",
    "3. **Alpha Tokens to Lowercase (78.4% Accuracy):**\n",
    "   - Converting alpha tokens to lowercase, with a slight difference in vocabulary size, resulted in **78.4% accuracy**.\n",
    "\n",
    "4. **Lowercasing Beginning of Sentences (77.9% Accuracy):**\n",
    "   - When only tokens at the beginning of sentences were converted to lowercase, accuracy dropped to **77.9%**.\n",
    "\n",
    "Among these cases, tokenizing alpha tokens and converting them to lowercase appears to be the best feature engineering method. Further accuracy improvements can be achieved by removing less frequent words, unknown words, and stop words. Let's explore these cases:\n",
    "\n",
    "1. **Ignoring Top 20 Stop Words (79.8% Accuracy):**\n",
    "   - The top 20 stop words, such as \"The,\" \"and,\" and \"with,\" were ignored to improve accuracy, resulting in **79.8%**.\n",
    "\n",
    "2. **Ignoring Less Frequent Words (81.1% Accuracy):**\n",
    "   - By further ignoring less frequent words, accuracy increased to **81.1%** in Naive Bayes  text classification.\n",
    "\n",
    "### Binary Multinomial Bayes\n",
    "\n",
    "In Binary Multinomial Bayes classification, the focus shifts to whether a word has occurred or not. This approach significantly improved accuracy to **82.6% compared to the initial accuracy of 77.9%** .\n",
    "\n",
    "### Naive Bayes Implementation without sklearn Library(From sctrach )\n",
    "\n",
    "An implementation of Naive Bayes using `spacytokenizer2` (considering only lower and alpha tokens) resulted in an accuracy of **80.66%**.\n",
    "\n",
    "These experiments demonstrate the impact of different feature engineering techniques on text classification accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
